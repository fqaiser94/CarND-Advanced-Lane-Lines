{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things remaining to do: \n",
    "\n",
    "- Clean up class code (and remove copies), probz best to get rid of class object\n",
    "- Calculate curvature correctly and display it\n",
    "- Calculate position of car in lane correctly and display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Line Detection\n",
    "***\n",
    "\n",
    "One important aspect of driving is for drivers to remain within their lane. This minimizes the chances of collisions with other cars. As humans, we are able to do this quite naturally. For computers, this isn't quite so easy. The first part of this is to detect driving lanes. In this project, we demonstrate one approach for detecting driving lanes using some well known computer vision techniques. Below, we describe each of these techniques in detail and show how they can be applied using a combination of Python and OpenCV to detect driving lanes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Admin\n",
    "\n",
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# image processing packages\n",
    "import cv2\n",
    "\n",
    "# visualization packages\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## General Approach \n",
    "\n",
    "There are a number of ways to detect driving lanes from images and the following is one such (relatively simple) approach: \n",
    "\n",
    "1. Camera calibration\n",
    "2. RGB to HSL\n",
    "3. Gaussian blur\n",
    "4. Thresholding\n",
    "5. Perspective transform\n",
    "6. Detect lane lines\n",
    "7. Measuring curvature\n",
    "8. Overlay detection on original image\n",
    "\n",
    "We'll start by developing a pipeline using individual images. Later we show how this can easily be extended to videos (which are really just a series of images). We will use the following sample images for developing and testing our pipelines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_dir = \"test_images/\"\n",
    "\n",
    "original_image_names = (\n",
    "    sorted(\n",
    "        os.listdir(test_img_dir)\n",
    "    )\n",
    ")\n",
    "\n",
    "original_image_names = list(\n",
    "    map(lambda name: test_img_dir + name, \n",
    "        original_image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_list(img_list, \n",
    "                    cols=2, \n",
    "                    fig_size=(10, 10),\n",
    "                    img_labels=original_image_names, \n",
    "                    cmap=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Show a list of images\n",
    "    \"\"\"\n",
    "    \n",
    "    img_count = len(img_list)\n",
    "    \n",
    "    rows = np.ceil(img_count / cols)\n",
    "    \n",
    "    plt.figure(figsize=fig_size)\n",
    "        \n",
    "    for i in range(0, img_count):\n",
    "        \n",
    "        img_name = img_labels[i]\n",
    "        \n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        \n",
    "        img = img_list[i]\n",
    "                \n",
    "        # no ticks on axes\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "        # plot title\n",
    "        plt.title(img_name)    \n",
    "        \n",
    "        # show plot \n",
    "        plt.imshow(img, cmap=cmap)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path): \n",
    "    \"\"\"\n",
    "    load images in RGB format\n",
    "    \"\"\"\n",
    "    \n",
    "    imgBGR = cv2.imread(image_path)\n",
    "    \n",
    "    imgRGB = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return imgRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = list(\n",
    "    map(lambda img: load_image(img), \n",
    "        original_image_names)\n",
    ")\n",
    "\n",
    "show_image_list(img_list = original_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few immediately obvious challenges we need to be mindful of while building a generalized pipeline for deteting lanes in these images:  \n",
    "\n",
    "1. While the lane lines are generally coloured white, they can also be yellow. \n",
    "2. While most of these images depict only light traffic, some of them have other cars in them and what's worse is that some cars are colored white! \n",
    "3. All of these images depict fairly straight patches of road. It is obviously possible for roads and therefore our lane lines to be curved.   \n",
    "\n",
    "These are good things to know. Let's start building our pipeline. For testing our pipeline, we'll use just the first image (shown below).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = original_images[0]\n",
    "\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Camera calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process by which cameras produce 2D images of 3D objects isn't perfect; the shape and size of objects can change. Below is an example of a distorted image taken by a camera (left) and the same image undistorted (right).  \n",
    "\n",
    "<figure>\n",
    " <img src=\"writeup_images/chessboard_undistort.jpg\"/>\n",
    " <figcaption>\n",
    " <p></p> \n",
    "     \n",
    "Before we can do anything, we need to first deal with this distortion. Specifically, we will use images of chessboards taken by our camera to figure out it's calibration. After that, we can undistort any image taken by our particular camera. This will allow us to get accurate and useful information out of them later on when perform lane line detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in camera calibration images from directory\n",
    "\n",
    "# img dir\n",
    "camera_cal_img_dir = \"camera_cal/\"\n",
    "\n",
    "# img names\n",
    "camera_cal_image_names = (\n",
    "    sorted(\n",
    "        os.listdir(camera_cal_img_dir)\n",
    "    )\n",
    ")\n",
    "\n",
    "# image paths\n",
    "camera_cal_image_paths = list(\n",
    "    map(lambda name: camera_cal_img_dir + name, \n",
    "        camera_cal_image_names)\n",
    ")\n",
    "\n",
    "# images\n",
    "camera_cal_images = list(\n",
    "    map(lambda img: load_image(img), \n",
    "        camera_cal_image_paths)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_image_list(\n",
    "#     img_list = camera_cal_images,\n",
    "#     img_labels = camera_cal_image_names, \n",
    "#     cols=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(camera_cal_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale images\n",
    "gray_camera_cal_images = [\n",
    "    cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in camera_cal_images\n",
    "]\n",
    "\n",
    "# set chessboard dimensions\n",
    "dims = (9,6)\n",
    "\n",
    "# find chessboard corners\n",
    "chessboard_corners = [\n",
    "    cv2.findChessboardCorners(image, dims, None) for image in gray_camera_cal_images\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify chessboard corners have been correctly identified\n",
    "# plt.imshow(\n",
    "#     cv2.drawChessboardCorners(\n",
    "#     camera_cal_images[0], \n",
    "#     dims, \n",
    "#     *chessboard_corners[0][::-1])\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correct camera matrix and distortion coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object points\n",
    "objp = np.zeros(\n",
    "    (np.prod(dims), 3), \n",
    "    np.float32\n",
    ")\n",
    "\n",
    "objp[:,:2] = np.mgrid[0:dims[0], 0:dims[1]].T.reshape(-1,2)\n",
    "\n",
    "objpoints = [\n",
    "    objp for corner_set in chessboard_corners if corner_set[0]\n",
    "]\n",
    "\n",
    "# image points\n",
    "imgpoints = [\n",
    "    corner_set[1] for corner_set in chessboard_corners if corner_set[0]\n",
    "]\n",
    "\n",
    "# extract camera calibration paramets\n",
    "ret, mtx, dist, rvecs, tvecs = (\n",
    "    cv2.calibrateCamera(\n",
    "        objpoints, \n",
    "        imgpoints, \n",
    "        gray_camera_cal_images[0].shape[::-1], \n",
    "        None, \n",
    "        None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort(img, mtx, dist): \n",
    "    \"\"\"\n",
    "    undistort camera images\n",
    "    \"\"\"\n",
    "    undistorted_img = cv2.undistort(\n",
    "        img, \n",
    "        mtx, \n",
    "        dist, \n",
    "        None, \n",
    "        mtx\n",
    "    )\n",
    "    \n",
    "    return undistorted_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_camera_cal_images = list(\n",
    "    map(lambda img: undistort(img, mtx, dist), \n",
    "        camera_cal_images)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_image_list(\n",
    "#     img_list = undistorted_camera_cal_images,\n",
    "#     img_labels = camera_cal_image_names, \n",
    "#     cols=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        camera_cal_images[0], undistorted_camera_cal_images[0], \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'original', 'undistorted image'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's undistort our actual driving images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_image = undistort(\n",
    "    img=test_image, \n",
    "    mtx=mtx, \n",
    "    dist=dist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        test_image, undistorted_image, \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'original', 'undistorted_image'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Convert image to HSL\n",
    "\n",
    "There are a number of color models, below is a brief introduction to two of the most popular models in use today.   \n",
    "\n",
    "#### RGB \n",
    "\n",
    "The most commonly used model is the RGB model which defines a color space in terms of three components:\n",
    "\n",
    "- Red, which ranges from 0-255\n",
    "- Green, which ranges from 0-255\n",
    "- Blue, which ranges from 0-255\n",
    "\n",
    "The RGB color model is additive i.e. Red, Green and Blue values are combined to reproduce other colors. \n",
    "\n",
    "<figure>\n",
    " <img src=\"writeup_images/rgb.png\"/>\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> RGB color model </p> \n",
    "\n",
    "#### HLS \n",
    "\n",
    "The HLS color space stands for:\n",
    "\n",
    "- Hue : the color type (such as red, blue, or yellow). Ranges from 0 to 360° in most applications (each value corresponds to one color : 0 is red, 45 is a shade of orange and 55 is a shade of yellow).\n",
    "- Lightness (also Luminance or Luminosity or Intensity). Ranges from 0 to 100% (from black to white).\n",
    "- Saturation : variation of the color depending on the lightness. Ranges from 0 to 100% (from the center of the black&white axis).\n",
    "\n",
    "\n",
    "<figure>\n",
    " <img src=\"writeup_images/hsl.png\"/>\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> HLS color model </p> \n",
    "     \n",
    "#### HSV \n",
    "\n",
    "The HSV color space stands for:\n",
    "\n",
    "- Hue : the color type (such as red, blue, or yellow). Ranges from 0 to 360° in most applications (each value corresponds to one color : 0 is red, 45 is a shade of orange and 55 is a shade of yellow).\n",
    "- Saturation : variation of the color depending on the lightness. Ranges from 0 to 100% (from the center of the black&white axis).  \n",
    "- Value: xxx\n",
    "\n",
    "<figure>\n",
    " <img src=\"writeup_images/hsv.png\"/>\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> HSV color model </p> \n",
    "\n",
    "\n",
    "There are other color models out there too including CMYK, and YUV. Like most images however, all of our images are in RBG format. So why are we talking about other color models? We suspect that it might be easier to isolate driving lanes in other color spaces than RGB. So let's have a look at our test images in HSL.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb(img_rgb, channel):\n",
    "    \"\"\"\n",
    "    extracts specified channel from RGB image\n",
    "    \"\"\"\n",
    "    \n",
    "    if channel=='r':\n",
    "        img_channel = img_rgb[:,:,0] \n",
    "    if channel=='g':\n",
    "        img_channel = img_rgb[:,:,1]\n",
    "    if channel=='b':\n",
    "        img_channel = img_rgb[:,:,2]\n",
    "    if channel==None:\n",
    "        img_channel = img_rgb\n",
    "    \n",
    "    return img_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hls(img, channel):\n",
    "    \"\"\"\n",
    "    converts RGB image to HSL and extracts specified channel\n",
    "    \"\"\"\n",
    "    img_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    \n",
    "    if channel=='h':\n",
    "        img_channel = img_hls[:,:,0] \n",
    "    if channel=='l':\n",
    "        img_channel = img_hls[:,:,1]\n",
    "    if channel=='s':\n",
    "        img_channel = img_hls[:,:,2]\n",
    "    if channel==None:\n",
    "        img_channel = img_hls\n",
    "    \n",
    "    return img_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsv(img, channel):\n",
    "    \"\"\"\n",
    "    converts RGB image to HSV and extracts specified channel\n",
    "    \"\"\"\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    if channel=='h':\n",
    "        img_channel = img_hsv[:,:,0] \n",
    "    if channel=='s':\n",
    "        img_channel = img_hsv[:,:,1]\n",
    "    if channel=='v':\n",
    "        img_channel = img_hsv[:,:,2]\n",
    "    if channel==None:\n",
    "        img_channel = img_hsv\n",
    "    \n",
    "    return img_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        rgb(undistorted_image, 'r'), rgb(undistorted_image, 'g'), rgb(undistorted_image, 'b'), \n",
    "        hls(undistorted_image, 'h'), hls(undistorted_image, 'l'), hls(undistorted_image, 's'), \n",
    "        hsv(undistorted_image, 'h'), hsv(undistorted_image, 's'), hsv(undistorted_image, 'v')\n",
    "    ],\n",
    "    img_labels=[\n",
    "        'r', 'g', 'b',\n",
    "        'h', 'l', 's', \n",
    "        'h', 's', 'v'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **s**aturation channel from HLS looks best, we'll go with that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_s_image = hls(img=undistorted_image, channel='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        test_image, hls_s_image, \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'original', 'hls_s'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of the S channel in HLS that we don't show here is that it is very robust to changing conditions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Gaussian blur\n",
    "\n",
    "Gaussian blur is a commonly used pre-processing technique that does exactly what it sounds like it does.   \n",
    "\n",
    "<figure>\n",
    " <img src=\"writeup_images/gaussian_blur.png\"/>\n",
    " <figcaption>\n",
    " <p></p>  \n",
    "     \n",
    "We're going to use Gaussian blur to reduce the number of edges we detect later on. We have to be careful though. Too much blurring and it will become difficult to find any lines. We can control the amount of blurring by adjusting our kernel size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur(img, kernel_size=5):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_image = gaussian_blur(hls_s_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        hls_s_image, blur_image, \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'HLS_S', 'Blurred'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although its a little difficult to see here, the sharp jagged edges of our lane lines have become smoother and more continuous. This should help in a few stages when we try to detect edges.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Thresholding\n",
    "\n",
    "Thresholding is one of the most basic approaches to image segmentation. Essentially, it involves turning pixels on/off based on whether a pixel property satisfies a given threshold condition. The end result are **binary** images, which by definition contain significantly less noise.   \n",
    "\n",
    "Below we explore a number of thresholding approaches.  \n",
    "\n",
    "### Color thresholding  \n",
    "\n",
    "Color thresholding is the easiest to understand. We've already explored thresholding individual RGB/HLS/HSV color channels. This time we'll threshold pixel values in our given channel. \n",
    "\n",
    "So for example, if we say we want to threshold for pixel values above 250 in an image, this means we  only want to retain pixels where the pixel value is greater than 250. In effect, we would end up retaining only the white elements of an image. Note that this isn't necessarily what we want given that lane lines can also be yellow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_threshold(img, thresh=(50, 255)):\n",
    "    \"\"\"\n",
    "    thresholds a given single channel image\n",
    "    \"\"\"\n",
    "    \n",
    "    binary_output = np.zeros_like(img)\n",
    "    binary_output[(img > thresh[0]) & (img <= thresh[1])] = 1\n",
    "    \n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds=[\n",
    "    (50, 255),\n",
    "    (75, 255),\n",
    "    (100, 255),\n",
    "    (125, 255),\n",
    "    (150, 255),\n",
    "    (175, 255)\n",
    "    ]\n",
    "\n",
    "show_image_list(\n",
    "    img_list=[color_threshold(blur_image, threshold) for threshold in thresholds],\n",
    "    img_labels=thresholds, \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "175, 255 it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient thresholding  \n",
    "\n",
    "Applying the Sobel operator to an image is a way of taking the derivative (i.e. gradient) of the image in the x or y direction. \n",
    "\n",
    "One way of thinking about how these operators work is to imagine a small window overlaid over a portion of your image. If the image is flat across that region, then the result (summing the element-wise product of the operator and corresponding image pixels) will be zero. If, instead, for example, you apply the Sobelx operator to a region of the image where values are rising from left to right, then the result will be positive, implying a positive derivative in the x direction. \n",
    "\n",
    "Taking the gradient in the y direction emphasizes edges closer to horizontal. \n",
    "\n",
    "\n",
    "\n",
    "Given that we expect our lane lines to be generally vertical, we'll see that taking the gradient in the y direction is more useful for our purposes.  \n",
    "\n",
    "If you play around with the thresholds a bit, you'll find the x-gradient does a cleaner job of picking up the lane lines, but you can see the lines in the y-gradient as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    \"\"\"\n",
    "    expects a single channel image\n",
    "    sobel_kernel must be an odd number, choose a larger odd number to smooth gradient measurements \n",
    "    \"\"\"\n",
    "    \n",
    "    # Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    if orient=='x': \n",
    "        sobel = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    elif orient=='y':\n",
    "        sobel = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "        \n",
    "    # Take the absolute value of the derivative or gradient\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    \n",
    "    # Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    \n",
    "    # Create a mask of 1's where the scaled gradient magnitude \n",
    "    # is > thresh_min and < thresh_max\n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    \n",
    "    return grad_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[\n",
    "    ('x', 3, 1, 255),\n",
    "    ('x', 3, 10, 255),\n",
    "    ('x', 3, 20, 255),\n",
    "    \n",
    "    ('x', 3, 50, 255),\n",
    "    ('x', 3, 100, 255),\n",
    "    ('x', 3, 200, 255)\n",
    "    ]\n",
    "\n",
    "show_image_list(\n",
    "    img_list=[\n",
    "        abs_sobel_thresh(img = blur_image, \n",
    "                         orient = param[0], \n",
    "                         sobel_kernel = param[1], \n",
    "                         thresh = (param[2], param[3])\n",
    "                        ) for param in params\n",
    "    ],\n",
    "    img_labels=params, \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the 'x' orientation is pretty useless for detecting driving lanes, irrespective of our threshold values.  \n",
    "\n",
    "Let's try again using the 'y' orientation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[\n",
    "    ('y', 11, 1, 255),\n",
    "    ('y', 11, 10, 255),\n",
    "    ('y', 11, 20, 255),\n",
    "    \n",
    "    ('y', 11, 50, 255),\n",
    "    ('y', 11, 100, 255),\n",
    "    ('y', 11, 200, 255)\n",
    "    ]\n",
    "\n",
    "show_image_list(\n",
    "    img_list=[\n",
    "        abs_sobel_thresh(\n",
    "            img = blur_image, \n",
    "            orient = param[0], \n",
    "            sobel_kernel = param[1], \n",
    "            thresh = (param[2], param[3])\n",
    "            ) for param in params\n",
    "    ],\n",
    "    img_labels=params, \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude thresholding  \n",
    "\n",
    "With the result of the last quiz, you can now take the gradient in x or y and set thresholds to identify pixels within a certain gradient range. If you play around with the thresholds a bit, you'll find the x-gradient does a cleaner job of picking up the lane lines, but you can see the lines in the y-gradient as well.\n",
    "\n",
    "In this next exercise, your goal is to apply a threshold to the overall magnitude of the gradient, in both x and y.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_threshold(img, sobel_kernel=3, thresh=(0, 255)):\n",
    "    \"\"\"\n",
    "    expects a single channel image\n",
    "    sobel_kernel must be an odd number, choose a larger odd number to smooth gradient measurements\n",
    "    \"\"\"\n",
    "    \n",
    "    # Take the derivative in x and y\n",
    "    sobel_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobel_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "\n",
    "    # Take the absolute value of the derivative or gradient\n",
    "    abs_sobel_xy = np.sqrt(np.square(sobel_x) + np.square(sobel_y))\n",
    "\n",
    "    # Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*abs_sobel_xy/np.max(abs_sobel_xy))\n",
    "    \n",
    "    # Create a mask of 1's where mag thresholds are met\n",
    "    mag_binary = np.zeros_like(scaled_sobel)\n",
    "    mag_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    \n",
    "    return mag_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[\n",
    "    (11, 1, 255),\n",
    "    (11, 5, 255),\n",
    "    (11, 10, 255),\n",
    "    (11, 15, 255),\n",
    "    (11, 20, 255),\n",
    "    (11, 25, 255),\n",
    "    ]\n",
    "\n",
    "show_image_list(\n",
    "    img_list=[\n",
    "        mag_threshold(\n",
    "            img = blur_image, \n",
    "            sobel_kernel = param[0], \n",
    "            thresh = (param[1], param[2])\n",
    "        ) for param in params\n",
    "    ],\n",
    "    img_labels=params, \n",
    "    cmap='gray',\n",
    "    cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5,25,150 <- looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction thresholding  \n",
    "\n",
    "When you play around with the thresholding for the gradient magnitude in the previous exercise, you find what you might expect, namely, that it picks up the lane lines well, but with a lot of other stuff detected too. Gradient magnitude is at the heart of Canny edge detection, and is why Canny works well for picking up all edges.\n",
    "\n",
    "In the case of lane lines, we're interested only in edges of a particular orientation. So now we will explore the direction, or orientation, of the gradient.\n",
    "\n",
    "The direction of the gradient is simply the inverse tangent (arctangent) of the y gradient divided by the x gradient:\n",
    "\n",
    "arctan(sobel \n",
    "y\n",
    "​\t /sobel \n",
    "x\n",
    "​\t ).\n",
    "\n",
    "Each pixel of the resulting image contains a value for the angle of the gradient away from horizontal in units of radians, covering a range of −π/2 to π/2. An orientation of 0 implies a horizontal line and orientations of +/−π/2 imply vertical lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    \"\"\"\n",
    "    expects a single channel image\n",
    "    sobel_kernel must be an odd number, choose a larger odd number to smooth gradient measurements\n",
    "    \"\"\"\n",
    "    \n",
    "    # Take the derivative in x and y\n",
    "    sobel_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobel_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    # Take the absolute value of the x and y gradients\n",
    "    abs_sobel_x = np.absolute(sobel_x)\n",
    "    abs_sobel_y = np.absolute(sobel_y)\n",
    "    \n",
    "    # Calculate the direction of the gradient \n",
    "    abs_grad_dir = np.arctan2(abs_sobel_y, abs_sobel_x)\n",
    "    \n",
    "    # Create a mask of where direction thresholds are met\n",
    "    dir_binary = np.zeros_like(abs_grad_dir)\n",
    "    dir_binary[(abs_grad_dir >= thresh[0]) & (abs_grad_dir <= thresh[1])] = 1\n",
    "    \n",
    "    return dir_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[\n",
    "    (5, 0.7, 1.3),\n",
    "    (5, 0.9, 1.3),\n",
    "    (5, 1.0, 1.3),\n",
    "    (9, 0.7, 1.3),\n",
    "    (9, 0.9, 1.3),\n",
    "    (9, 1.0, 1.3),\n",
    "    ]\n",
    "\n",
    "show_image_list(\n",
    "    img_list=[\n",
    "        dir_threshold(\n",
    "            img = blur_image, \n",
    "            sobel_kernel = param[0], \n",
    "            thresh = (param[1], param[2])\n",
    "        ) for param in params\n",
    "    ],\n",
    "    img_labels=params, \n",
    "    cmap='gray',\n",
    "    cols=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining multiple thresholds  \n",
    "\n",
    "We can combine multiple thresholds using an AND operation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_thresholds(images): \n",
    "    \"\"\"\n",
    "    combines images thresholded in different ways\n",
    "    \"\"\"\n",
    "    \n",
    "    combined = np.zeros_like(images[1])\n",
    "    \n",
    "    length = len(images)\n",
    "    \n",
    "    if length==2:\n",
    "        combined[(images[0]==1) & (images[1]==1)] = 1\n",
    "        \n",
    "    if length==3:\n",
    "        combined[(images[0]==1) & \n",
    "                 (images[1]==1) &\n",
    "                 (images[2]==1)\n",
    "                ] = 1\n",
    "        \n",
    "    if length==4:\n",
    "        combined[(images[0]==1) & \n",
    "                 (images[1]==1) &\n",
    "                 (images[2]==1) &\n",
    "                 (images[3]==1)\n",
    "                ] = 1\n",
    "        \n",
    "    if length==5:\n",
    "        combined[(images[0]==1) & \n",
    "                 (images[1]==1) &\n",
    "                 (images[2]==1) &\n",
    "                 (images[3]==1) &\n",
    "                 (images[4]==1)\n",
    "                ] = 1\n",
    "        \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[\n",
    "    color_threshold(\n",
    "        img=blur_image, \n",
    "        thresh=(50,255)\n",
    "    ), \n",
    "#      abs_sobel_thresh(\n",
    "#         img = blur_image, \n",
    "#         orient = 'y', \n",
    "#         sobel_kernel = 11, \n",
    "#         thresh = (10, 255)\n",
    "#     ), \n",
    "     mag_threshold(\n",
    "         img = blur_image, \n",
    "         sobel_kernel = 11, \n",
    "         thresh = (10, 255)\n",
    "     ), \n",
    "     dir_threshold(\n",
    "         img = blur_image, \n",
    "         sobel_kernel = 9, \n",
    "         thresh = (0.7, 1.3)\n",
    "     )\n",
    "    \n",
    "]\n",
    "\n",
    "# show_image_list(\n",
    "#     img_list=[\n",
    "#         combine_thresholds(\n",
    "#             images = [param for param in params]\n",
    "#         )\n",
    "#     ],\n",
    "#     img_labels=['Preprocessed'], \n",
    "#     cmap='gray',\n",
    "#     cols=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_image = combine_thresholds(\n",
    "    images = [param for param in params]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        test_image, threshold_image, \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'original', 'threshold_image'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking pretty good.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Perspective Transform  \n",
    "\n",
    "The image we have has been de-noised pretty well. We can now start thinking about finding lane lines. \n",
    "\n",
    "Because of where our camera is placed, we have a front-facing view, or perspective, of the road. This means that the lane in front of us looks smaller the further away it gets from our camera. \n",
    "\n",
    "Perspective transform is a technique for warping an image to get different views or perspectives. It would be useful for us if we could zoom in on the further away objects in our image, i.e. get a top-down view of the road. Some of our tasks later on will be easier to perform on this kind of view. \n",
    "\n",
    "For example, fitting a line and calculating the lane curvature will be easier with this view later on. Currently you'll notice that the left lane line is leaning towards the right in our test image. The right lane line is leaning towards the left. In fact, in the image it looks like the two lane lines will eventually converge. However, the reality is that they are both parallel. A top down perspective transform will show as much.  \n",
    "\n",
    "Although not relevant for this project, this perspective is also useful for matching up a car's location with a map.  \n",
    "\n",
    "Applying a perspective transform is very similar to what we did for undistorting an image. The only difference is that tnstead of mapping object points to image points, this time we'll be mapping points on an image to different desired points on an image with a new perspective.  \n",
    "\n",
    "It should be noted that perspective transforms can be used for more than just creating a bird’s eye view representation of an image.  \n",
    "\n",
    "<figure>\n",
    " <img src=\"writeup_images/perspective_transform.jpg\"/>\n",
    " <figcaption>\n",
    " <p></p>  \n",
    "     \n",
    "First, we need to figure out what 4 points we want to warp our image about.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a trapezoidal region of interest about our lane\n",
    "\n",
    "# where we want our trapezoid to start and end along the y axis\n",
    "min_y = 720\n",
    "max_y = 455\n",
    "\n",
    "# define x,y for 4 points\n",
    "left_x1 = 190 \n",
    "left_y1 = min_y\n",
    "\n",
    "left_x2 = 585\n",
    "left_y2 = max_y\n",
    "\n",
    "right_x1 = 705\n",
    "right_y1 = max_y\n",
    "\n",
    "right_x2 = 1130\n",
    "right_y2 = min_y\n",
    "\n",
    "# define array of 4 points\n",
    "# the order matters!\n",
    "# first entry is the top-left\n",
    "# second entry is the top-right\n",
    "# third is the bottom-right\n",
    "# fourth is the bottom-left\n",
    "\n",
    "src = np.float32([ \n",
    "    [left_x2, left_y2],\n",
    "    [right_x1, right_y1],\n",
    "    [right_x2, right_y2],\n",
    "    [left_x1, left_y1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = threshold_image.shape\n",
    "\n",
    "# temp copy of image\n",
    "temp = threshold_image.copy()\n",
    "\n",
    "color = [255, 0, 0]\n",
    "thickness = 10\n",
    "\n",
    "# add lines\n",
    "cv2.line(\n",
    "    img = temp, \n",
    "    pt1 = (left_x1, left_y1), \n",
    "    pt2 = (left_x2, left_y2), \n",
    "    color = color, \n",
    "    thickness = thickness\n",
    ")\n",
    "\n",
    "cv2.line(\n",
    "    img = temp, \n",
    "    pt1 = (left_x2, left_y2), \n",
    "    pt2 = (right_x1, right_y1), \n",
    "    color = color, \n",
    "    thickness = thickness\n",
    ")\n",
    "\n",
    "cv2.line(\n",
    "    img = temp, \n",
    "    pt1 = (right_x1, right_y1), \n",
    "    pt2 = (right_x2, right_y2), \n",
    "    color = color, \n",
    "    thickness = thickness\n",
    ")\n",
    "\n",
    "cv2.line(\n",
    "    img = temp, \n",
    "    pt1 = (right_x2, right_y2), \n",
    "    pt2 = (left_x1, left_y1), \n",
    "    color = color, \n",
    "    thickness = thickness\n",
    ")\n",
    "\n",
    "# plt.imshow(threshold_image, cmap='gray')\n",
    "# plt.imshow(temp, alpha = 0.5)\n",
    "\n",
    "plt.imshow(threshold_image, cmap='gray')\n",
    "plt.scatter(src[:,0], src[:,1], color='red', alpha = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a matching set of 4 (destination) points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = threshold_image.shape[1]\n",
    "h = threshold_image.shape[0]\n",
    "\n",
    "# offset from the sides\n",
    "offset = 200\n",
    "\n",
    "# first entry is the top-left\n",
    "# second entry is the top-right\n",
    "# third is the bottom-right\n",
    "# fourth is the bottom-left\n",
    "dst = np.float32([\n",
    "    [offset, 0],\n",
    "    [w - offset, 0],\n",
    "    [w - offset, h], \n",
    "    [offset, h]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the perspective transform, M, given source and destination points\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "# Compute the inverse perspective transform\n",
    "# We will need this later on when we want to 'unwarp' our image\n",
    "Minv = cv2.getPerspectiveTransform(dst, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustPerspective(img, M):\n",
    "    \"\"\"\n",
    "    Adjust an image's perspective using the transformation matrix M\n",
    "    \"\"\"\n",
    "    \n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags = cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped = adjustPerspective(\n",
    "    img=threshold_image\n",
    "    , M=M\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(warped, cmap='gray')\n",
    "plt.scatter(dst[:,0], dst[:,1], color='red', alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        threshold_image, warped, \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'threshold_image', 'top down view'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Detect lane line pixels\n",
    "\n",
    "We now have a thresholded warped image. We can now actually start mapping out the lane lines! We're going to try a number of ways to do this. \n",
    "\n",
    "### Peaks in a histogram\n",
    "\n",
    "We have a binary image where the lane lines stand out pretty clearly. However, we still need to decide explicitly which pixels are part of the lines and also which pixels belong to the left and right lines respectively.  \n",
    "\n",
    "One way of doing this is using a histogram of the total of the pixel values in each 'column' of pixels.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = np.ones(35)\n",
    "\n",
    "plt.imshow(warped, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "total_sum = np.sum(warped, axis=0)\n",
    "plt.plot(total_sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the histogram peaks at exactly the same range of x values as our lane line pixels. This is because in our thresholded binary image, pixels are either 0 or 1, so the two most prominent peaks in this histogram will be good indicators of the x coordinates of the base of the lane lines. We will use these as starting points for where to search for the lane lines. \n",
    "\n",
    "Now we can use a sliding window, placed around the line centers, to find and follow the lines up to the top of the frame.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you've got a warped binary image called binary_warped and you want to find which \"hot\" pixels are associated with the lane lines. Here's a basic implementation of the method shown in the animation above. You should think about how you could improve this implementation to make sure you can find the lines as robustly as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lane_lines(img, nwindows=9):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    Arguments: \n",
    "        img: \n",
    "            must be binary\n",
    "            must show a top down view of road\n",
    "        nwindows: \n",
    "            number of sliding windows to use\n",
    "    Returns: \n",
    "        left_fit: \n",
    "        right_fit:\n",
    "    \"\"\"\n",
    "\n",
    "    # Assuming you have created a warped binary image called \"img\"\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    # why bottom half? Cause lane line could be curving\n",
    "    # and all we want to figure out is where to start drawing \n",
    "    # our lane lines from at the bottom of the image\n",
    "    # this isn't using any histogram function\n",
    "    # all its doing is returning an (ordered) array of the sum of each column in our image\n",
    "    histogram = np.sum(img[int(img.shape[0]/2):,:], axis=0)\n",
    "\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((img, img, img))*255\n",
    "\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting points for our left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # determine based on where we have the most pixels in a column\n",
    "    # best_defined_lane = np.argmax(histogram)\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "\n",
    "    # Set height of windows\n",
    "    window_height = np.int(img.shape[0]/nwindows)\n",
    "\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = img.shape[0] - (window+1)*window_height\n",
    "        win_y_high = img.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(\n",
    "            out_img,\n",
    "            (win_xleft_low,win_y_low),\n",
    "            (win_xleft_high,win_y_high),\n",
    "            (0,255,0), \n",
    "            2\n",
    "        ) \n",
    "\n",
    "        cv2.rectangle(\n",
    "            out_img,\n",
    "            (win_xright_low,win_y_low),\n",
    "            (win_xright_high,win_y_high),\n",
    "            (0,255,0), \n",
    "            2\n",
    "        ) \n",
    "\n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = (\n",
    "            (nonzeroy >= win_y_low) \n",
    "            & (nonzeroy < win_y_high) \n",
    "            & (nonzerox >= win_xleft_low) \n",
    "            & (nonzerox < win_xleft_high)\n",
    "        ).nonzero()[0]\n",
    "\n",
    "        good_right_inds = (\n",
    "            (nonzeroy >= win_y_low) \n",
    "            & (nonzeroy < win_y_high) \n",
    "            & (nonzerox >= win_xright_low) \n",
    "            & (nonzerox < win_xright_high)\n",
    "        ).nonzero()[0] \n",
    "\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "    \n",
    "    # Set minimum number of pixels found to create a line\n",
    "    # if a lane is curving this might be too extreme\n",
    "    # TODO: look for number of pixels in a range of adjacent x bins\n",
    "    min_line_pix = 300\n",
    "    \n",
    "    # if the number of pixels in a given lane is too low to get a good line\n",
    "    # use other lane if that is better\n",
    "    if (histogram[rightx_base] < min_line_pix) & (histogram[leftx_base] > min_line_pix): \n",
    "        rightx = leftx + (midpoint + leftx_base)\n",
    "        righty = lefty\n",
    "\n",
    "    if (histogram[leftx_base] < min_line_pix) & (histogram[rightx_base] > min_line_pix): \n",
    "        leftx = rightx + (midpoint + lrightx_base)\n",
    "        lefty = lefty\n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0])\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    color = (255,255,255)\n",
    "    \n",
    "    # draw left line\n",
    "    pts = (list(zip(left_fitx, ploty)))\n",
    "        \n",
    "    cv2.polylines(\n",
    "        out_img,\n",
    "        [np.int32(pts)],\n",
    "        False,\n",
    "        color, \n",
    "        10\n",
    "    )\n",
    "    \n",
    "    pts = (list(zip(right_fitx, ploty)))\n",
    "    \n",
    "    # draw right line\n",
    "    cv2.polylines(\n",
    "        out_img,\n",
    "        [np.int32(pts)],\n",
    "        False,\n",
    "        color, \n",
    "        10\n",
    "    )\n",
    "        \n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_lanes = fit_lane_lines(img=warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        warped, detected_lanes \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'warped', 'detected lanes'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we've clearly separated out the left and right lane pixels (coloured red and blue respectively) and we've got lines fitting our lane lines almost perfectly.   \n",
    "\n",
    "We can be a bit more efficient about this though from a videos perspective. After the first frame, we'll know where the lane lines are. Instead of doing a blind search again for the lane lines, we can instead just search in a margin around the previous line position. A class object is more useful here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lane_line_detector:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        midpoint_lane_x = None\n",
    "        self.left_lane_x = None\n",
    "        self.right_lane_x = None\n",
    "        \n",
    "        self.left_fit = None\n",
    "        self.right_fit = None\n",
    "        \n",
    "        self.counter = 1\n",
    "        \n",
    "        # if the first frame of video has been processed\n",
    "        self.first_frame_processed = False  \n",
    "    \n",
    "    def lane_position(self, img): \n",
    "        \n",
    "        # Assuming you have created a warped binary image called \"img\"\n",
    "        # Take a histogram of the bottom half of the image\n",
    "        # why bottom half? Cause lane line could be curving\n",
    "        # and all we want to figure out is where to start drawing \n",
    "        # our lane lines from at the bottom of the image\n",
    "        # this isn't using any histogram function\n",
    "        # all its doing is returning an (ordered) array of the sum of each column in our image\n",
    "        histogram = np.sum(img[int(img.shape[0]/2):,:], axis=0)\n",
    "\n",
    "        # find the peak of the left and right halves of the histogram\n",
    "        # these will be the starting points for our left and right lines\n",
    "        midpoint_x = np.int(histogram.shape[0]/2)\n",
    "        left_x = np.argmax(histogram[:midpoint_x])\n",
    "        right_x = np.argmax(histogram[midpoint_x:]) + midpoint_x\n",
    "        \n",
    "        # save\n",
    "        self.midpoint_lane_x = midpoint_x\n",
    "        self.left_lane_x = left_x\n",
    "        self.right_lane_x = right_x\n",
    "        \n",
    "    def identify_lane_pixels(\n",
    "        self,\n",
    "        nonzerox, \n",
    "        nonzeroy, \n",
    "        left_lane_inds, \n",
    "        right_lane_inds, \n",
    "        histogram\n",
    "    ):\n",
    "        # Set minimum number of pixels found to create a line\n",
    "        # if a lane is curving this might be too extreme\n",
    "        # TODO: look for number of pixels in a range of adjacent x bins\n",
    "        min_line_pix = 300\n",
    "        \n",
    "        # Extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "        # if the number of pixels in a given lane is too low to get a good line\n",
    "        # use other lane if that is better\n",
    "        if ((histogram[self.right_lane_x] < min_line_pix) \n",
    "            & (histogram[self.left_lane_x] > min_line_pix)): \n",
    "            rightx = leftx + (self.midpoint_lane_x + self.left_lane_x)\n",
    "            righty = lefty\n",
    "\n",
    "        if ((histogram[self.left_lane_x] < min_line_pix) \n",
    "            & (histogram[self.right_lane_x] > min_line_pix)): \n",
    "            leftx = rightx + (self.midpoint_lane_x + self.right_lane_x)\n",
    "            lefty = lefty\n",
    "\n",
    "        # Fit a second order polynomial to each\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "        return left_fit, right_fit\n",
    "            \n",
    "        \n",
    "    def fit_lane_lines(self, img, nwindows=9):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        Arguments: \n",
    "            img: \n",
    "                must be binary\n",
    "                must show a top down view of road\n",
    "            nwindows: \n",
    "                number of sliding windows to use\n",
    "        Returns: \n",
    "            left_fit: \n",
    "            right_fit:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create an output image to draw on and  visualize the result\n",
    "        out_img = np.dstack((img, img, img))*255\n",
    "\n",
    "        # Set height of windows\n",
    "        window_height = np.int(img.shape[0]/nwindows)\n",
    "\n",
    "        # Identify the x and y positions of all nonzero pixels in the image\n",
    "        nonzero = img.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "\n",
    "        # Set the width of the windows +/- margin\n",
    "        margin = 100\n",
    "\n",
    "        # Set minimum number of pixels found to recenter window\n",
    "        minpix = 50\n",
    "        \n",
    "        # Assuming you have created a warped binary image called \"img\"\n",
    "        # Take a histogram of the bottom half of the image\n",
    "        # why bottom half? Cause lane line could be curving\n",
    "        # and all we want to figure out is where to start drawing \n",
    "        # our lane lines from at the bottom of the image\n",
    "        # this isn't using any histogram function\n",
    "        # all its doing is returning an (ordered) array of the sum of each column in our image\n",
    "        histogram = np.sum(img[int(img.shape[0]/2):,:], axis=0)\n",
    "        \n",
    "        # Find the peak of the left and right halves of the histogram\n",
    "        # These will be the starting points for our left and right lines\n",
    "        if not self.first_frame_processed:\n",
    "            \n",
    "            self.lane_position(img)\n",
    "\n",
    "            # get lane position variables\n",
    "            midpoint = self.midpoint_lane_x\n",
    "            leftx_base = self.left_lane_x\n",
    "            rightx_base = self.right_lane_x\n",
    "            \n",
    "            # Create empty lists to receive left and right lane pixel indices\n",
    "            left_lane_inds = []\n",
    "            right_lane_inds = []\n",
    "            \n",
    "            # Current positions to be updated for each window\n",
    "            leftx_current = leftx_base\n",
    "            rightx_current = rightx_base\n",
    "\n",
    "            # Step through the windows one by one\n",
    "            for window in range(nwindows):\n",
    "\n",
    "                # Identify window boundaries in x and y (and right and left)\n",
    "                win_y_low = img.shape[0] - (window+1)*window_height\n",
    "                win_y_high = img.shape[0] - window*window_height\n",
    "                win_xleft_low = leftx_current - margin\n",
    "                win_xleft_high = leftx_current + margin\n",
    "                win_xright_low = rightx_current - margin\n",
    "                win_xright_high = rightx_current + margin\n",
    "\n",
    "                # Draw the windows on the visualization image\n",
    "                cv2.rectangle(\n",
    "                    out_img,\n",
    "                    (win_xleft_low,win_y_low),\n",
    "                    (win_xleft_high,win_y_high),\n",
    "                    (0,255,0), \n",
    "                    2\n",
    "                ) \n",
    "\n",
    "                cv2.rectangle(\n",
    "                    out_img,\n",
    "                    (win_xright_low,win_y_low),\n",
    "                    (win_xright_high,win_y_high),\n",
    "                    (0,255,0), \n",
    "                    2\n",
    "                ) \n",
    "\n",
    "                # Identify the nonzero pixels in x and y within the window\n",
    "                good_left_inds = (\n",
    "                    (nonzeroy >= win_y_low) \n",
    "                    & (nonzeroy < win_y_high) \n",
    "                    & (nonzerox >= win_xleft_low) \n",
    "                    & (nonzerox < win_xleft_high)\n",
    "                ).nonzero()[0]\n",
    "\n",
    "                good_right_inds = (\n",
    "                    (nonzeroy >= win_y_low) \n",
    "                    & (nonzeroy < win_y_high) \n",
    "                    & (nonzerox >= win_xright_low) \n",
    "                    & (nonzerox < win_xright_high)\n",
    "                ).nonzero()[0] \n",
    "\n",
    "                # Append these indices to the lists\n",
    "                left_lane_inds.append(good_left_inds)\n",
    "                right_lane_inds.append(good_right_inds)\n",
    "\n",
    "                # If you found > minpix pixels, recenter next window on their mean position\n",
    "                if len(good_left_inds) > minpix:\n",
    "                    leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "\n",
    "                if len(good_right_inds) > minpix:        \n",
    "                    rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "            # Concatenate the arrays of indices\n",
    "            left_lane_inds = np.concatenate(left_lane_inds)\n",
    "            right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "            self.left_fit, self.right_fit = self.identify_lane_pixels(\n",
    "                nonzerox, \n",
    "                nonzeroy, \n",
    "                left_lane_inds, \n",
    "                right_lane_inds, \n",
    "                histogram\n",
    "            )            \n",
    "            \n",
    "        else: \n",
    "            \n",
    "            # Assuming you have already a good fit for your lane lines from \n",
    "            # the first frame processed\n",
    "            # we don't need to do another blind search\n",
    "            \n",
    "            nonzero = img.nonzero()\n",
    "            nonzeroy = np.array(nonzero[0])\n",
    "            nonzerox = np.array(nonzero[1])\n",
    "\n",
    "            margin = 100\n",
    "            \n",
    "            left_fit = self.left_fit\n",
    "            right_fit = self.right_fit\n",
    "            \n",
    "            left_lane_inds = (\n",
    "                (nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - margin)) \n",
    "                & (nonzerox < (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin))\n",
    "            ) \n",
    "\n",
    "            right_lane_inds = (\n",
    "                (nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] - margin)) \n",
    "                & (nonzerox < (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin))\n",
    "            )  \n",
    "\n",
    "            self.left_fit, self.right_fit = self.identify_lane_pixels(\n",
    "                nonzerox, \n",
    "                nonzeroy, \n",
    "                left_lane_inds, \n",
    "                right_lane_inds, \n",
    "                histogram\n",
    "            )   \n",
    "            \n",
    "        # at this point, we should have left_fit and right_fit\n",
    "        # regardless of whether this is the first frame or not\n",
    "        \n",
    "        # Generate x and y values for plotting\n",
    "        ploty = np.linspace(0, img.shape[0]-1, img.shape[0])\n",
    "        \n",
    "        left_fitx = (\n",
    "            self.left_fit[0]*ploty**2 + \n",
    "            self.left_fit[1]*ploty + \n",
    "            self.left_fit[2]\n",
    "        )\n",
    "        \n",
    "        right_fitx = (\n",
    "            self.right_fit[0]*ploty**2 + \n",
    "            self.right_fit[1]*ploty + \n",
    "            self.right_fit[2]\n",
    "        )\n",
    "\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "        color = (255,255,255)\n",
    "\n",
    "        # draw left line\n",
    "        pts = (list(zip(left_fitx, ploty)))\n",
    "\n",
    "        cv2.polylines(\n",
    "            out_img,\n",
    "            [np.int32(pts)],\n",
    "            False,\n",
    "            color, \n",
    "            10\n",
    "        )\n",
    "\n",
    "        pts = (list(zip(right_fitx, ploty)))\n",
    "\n",
    "        # draw right line\n",
    "        cv2.polylines(\n",
    "            out_img,\n",
    "            [np.int32(pts)],\n",
    "            False,\n",
    "            color, \n",
    "            10\n",
    "        )\n",
    "        \n",
    "        if self.first_frame_processed:\n",
    "        \n",
    "            # Generate a polygon to illustrate the search window area\n",
    "            # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "            \n",
    "            search_area_img = np.zeros_like(out_img)\n",
    "            \n",
    "            left_line_window1 = np.array(\n",
    "                [np.transpose(np.vstack([left_fitx-margin, ploty]))]\n",
    "            )\n",
    "            \n",
    "            left_line_window2 = np.array(\n",
    "                [np.flipud(np.transpose(np.vstack([left_fitx + margin, ploty])))]\n",
    "            )\n",
    "            \n",
    "            left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "            \n",
    "            right_line_window1 = np.array(\n",
    "                [np.transpose(np.vstack([right_fitx-margin, ploty]))]\n",
    "            )\n",
    "            \n",
    "            right_line_window2 = np.array(\n",
    "                [np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))]\n",
    "            )\n",
    "            \n",
    "            right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "            # Draw the lane onto the warped blank image\n",
    "            cv2.fillPoly(search_area_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "            cv2.fillPoly(search_area_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "            \n",
    "            out_img = cv2.addWeighted(out_img, 1, search_area_img, 0.2, 0)\n",
    "\n",
    "        self.first_frame_processed = True\n",
    "        \n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_line_detector = Lane_line_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        lane_line_detector.fit_lane_lines(warped), lane_line_detector.fit_lane_lines(warped) \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'detected lane lines in first frame', 'detected lane lines in second frame'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, you can see that we didn't use the sliding windows approach in the second frame.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window search \n",
    "\n",
    "Another way to approach the sliding window method is to apply a convolution, which will \n",
    "maximize the number of \"hot\" pixels in each window. A convolution is the summation of the product of two separate signals, in our case the window template and the vertical slice of the pixel image.\n",
    "\n",
    "You slide your window template across the image from left to right and any overlapping values are summed together, creating the convolved signal. The peak of the convolved signal is where there was the highest overlap of pixels and the most likely position for the lane marker.\n",
    "\n",
    "Now let's try using convolutions to find the best window center positions in a thresholded road image. The code below allows you to experiment with using convolutions for a sliding window search function. Go ahead and give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def window_mask(width, height, img_ref, center, level):\n",
    "#     output = np.zeros_like(img_ref)\n",
    "#     output[int(img_ref.shape[0] - (level + 1)*height):int(img_ref.shape[0] - level*height), max(0, int(center-width)):min(int(center+width), img_ref.shape[1])] = 1\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class tracker():\n",
    "    \n",
    "#     def __init__(self, Mywindow_width, Mywindow_height, Mymargin, Mysmooth_factor = 15):\n",
    "        \n",
    "#         self.recent_centers = []\n",
    "#         self.window_width = Mywindow_width\n",
    "#         self.window_height = Mywindow_height\n",
    "#         self.margin = Mymargin\n",
    "#         self.smooth_factor = Mysmooth_factor\n",
    "        \n",
    "#     def find_window_centroids(self, warped):\n",
    "        \n",
    "#         window_width = self.window_width\n",
    "#         window_height = self.window_height\n",
    "#         margin = self.margin\n",
    "        \n",
    "#         window_centroids = []\n",
    "#         window = np.ones(window_width)\n",
    "        \n",
    "#         l_sum = np.sum(warped[int(3*warped.shape[0]/4):,:int(warped.shape[1]/2)], axis=0)\n",
    "#         l_center = np.argmax(np.convolve(window, l_sum)) - window_width/2\n",
    "#         r_sum = np.sum(warped[int(3*warped.shape[0]/4):,int(warped.shape[1]/2):], axis=0)\n",
    "#         r_center = np.argmax(np.convolve(window, r_sum)) - window_width/2 + int(warped.shape[1]/2)\n",
    "        \n",
    "#         window_centroids.append((l_center, r_center))\n",
    "        \n",
    "#         for level in range(1, (int)(warped.shape[0]/window_height)):\n",
    "#             image_layer = np.sum(warped[int(warped.shape[0] - (level+1)*window_height):int(warped.shape[0]-level*window_height),:], axis=0)\n",
    "#             conv_signal = np.convolve(window, image_layer)\n",
    "            \n",
    "#             offset = window_width/2\n",
    "#             l_min_index = int(max(l_center + offset - margin, 0))\n",
    "#             l_max_index = int(min(l_center + offset + margin, warped.shape[1]))\n",
    "#             l_center = np.argmax(conv_signal[l_min_index:l_max_index]) + l_min_index - offset\n",
    "            \n",
    "#             r_min_index = int(max(r_center + offset - margin, 0))\n",
    "#             r_max_index = int(min(r_center + offset + margin, warped.shape[1]))\n",
    "#             r_center = np.argmax(conv_signal[r_min_index: r_max_index]) + r_min_index - offset\n",
    "            \n",
    "#             window_centroids.append((l_center, r_center))\n",
    "            \n",
    "#         self.recent_centers.append(window_centroids)\n",
    "        \n",
    "#         return np.average(self.recent_centers[-self.smooth_factor:], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_width = 20\n",
    "# window_height = 100\n",
    "# curve_centers = tracker(Mywindow_width = 35, Mywindow_height = 80, Mymargin = 50, Mysmooth_factor = 100)\n",
    "\n",
    "# window_centroids = curve_centers.find_window_centroids (warped)\n",
    "\n",
    "# l_points = np.zeros_like(warped)\n",
    "# r_points = np.zeros_like(warped)\n",
    "\n",
    "# leftx = []\n",
    "# rightx = []\n",
    "\n",
    "# for level in range(0, len(window_centroids)):\n",
    "\n",
    "#     leftx.append(window_centroids[level][0])\n",
    "#     rightx.append(window_centroids[level][1])\n",
    "\n",
    "#     l_mask = window_mask(window_width, window_height, warped, window_centroids[level][0], level)\n",
    "#     r_mask = window_mask(window_width, window_height, warped, window_centroids[level][1], level)\n",
    "\n",
    "#     l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "#     r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "# template = np.array(r_points + l_points, np.uint8)\n",
    "# zero_channel = np.zeros_like(template)\n",
    "# template = np.array(cv2.merge((zero_channel, template, zero_channel)), np.uint8)\n",
    "# warpage = np.array(cv2.merge((warped, warped, warped)), np.uint8)\n",
    "# detected_lanes = cv2.addWeighted(warpage, 1, template, 0.5, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(detected_lanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Measuring curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversions in x and y from pixels space to meters\n",
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meters per pixel in x dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Overlay detection on original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lane_line_detector:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        midpoint_lane_x = None\n",
    "        self.left_lane_x = None\n",
    "        self.right_lane_x = None\n",
    "        \n",
    "        self.left_lane_y = None\n",
    "        self.right_lane_y = None\n",
    "        \n",
    "        self.left_coefficients = None\n",
    "        self.right_coefficients = None\n",
    "        \n",
    "        self.first_frame_processed = False  \n",
    "    \n",
    "    def get_lane_position(self, img): \n",
    "        \n",
    "        \"\"\"\n",
    "        Get position of left lane line, right lane line, middle point of lane\n",
    "        \"\"\"\n",
    "        \n",
    "        # Assuming you have created a warped binary image called \"img\"\n",
    "        # Take a histogram of the bottom half of the image\n",
    "        # why bottom half? Cause lane line could be curving\n",
    "        # and all we want to figure out is where to start drawing \n",
    "        # our lane lines from at the bottom of the image\n",
    "        # this isn't using any histogram function\n",
    "        # all its doing is returning an (ordered) array of the sum of each column in our image\n",
    "        histogram = np.sum(img[int(img.shape[0]/2):,:], axis=0)\n",
    "\n",
    "        # find the peak of the left and right halves of the histogram\n",
    "        # these will be the starting points for our left and right lines\n",
    "        midpoint_x = np.int(histogram.shape[0]/2)\n",
    "        left_x = np.argmax(histogram[:midpoint_x])\n",
    "        right_x = np.argmax(histogram[midpoint_x:]) + midpoint_x\n",
    "        \n",
    "        # save\n",
    "        self.midpoint_lane_x = midpoint_x\n",
    "        self.left_lane_x = left_x\n",
    "        self.right_lane_x = right_x\n",
    "        \n",
    "    def identify_lane_pixels(\n",
    "        self,\n",
    "        nonzerox, \n",
    "        nonzeroy, \n",
    "        left_lane_inds, \n",
    "        right_lane_inds):\n",
    "        \"\"\"\n",
    "        Identify pixels as left or right lane pixels\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds] \n",
    "        \n",
    "        return leftx, lefty, rightx, righty\n",
    "    \n",
    "    def calc_lane_line_coefficients(\n",
    "        self,\n",
    "        nonzerox, \n",
    "        nonzeroy,\n",
    "        left_lane_inds, \n",
    "        right_lane_inds, \n",
    "        histogram,\n",
    "        ploty, \n",
    "        space='p'\n",
    "    ): \n",
    "        \"\"\"\n",
    "        Get lane line coefficients\n",
    "        \n",
    "        Arguments: \n",
    "            space: \n",
    "                pixel space ('p') or real world space ('m' as metres)\n",
    "        \"\"\"\n",
    "        \n",
    "        leftx, lefty, rightx, righty = self.identify_lane_pixels(\n",
    "            nonzerox, \n",
    "            nonzeroy, \n",
    "            left_lane_inds, \n",
    "            right_lane_inds\n",
    "        )\n",
    "        \n",
    "        # Set minimum number of pixels found to create a line\n",
    "        # if a lane is curving this might be too extreme\n",
    "        # TODO: look for number of pixels in a range of adjacent x bins\n",
    "        min_line_pix = 300\n",
    "        \n",
    "        # if the number of pixels in a given lane is too low to get a good line\n",
    "        # use other lane if that is better\n",
    "        if ((histogram[self.right_lane_x] < min_line_pix) \n",
    "            & (histogram[self.left_lane_x] > min_line_pix)): \n",
    "            rightx = leftx + (self.midpoint_lane_x + self.left_lane_x)\n",
    "            righty = lefty\n",
    "\n",
    "        if ((histogram[self.left_lane_x] < min_line_pix) \n",
    "            & (histogram[self.right_lane_x] > min_line_pix)): \n",
    "            leftx = rightx + (self.right_lane_x - self.midpoint_lane_x)\n",
    "            lefty = righty\n",
    "\n",
    "        # Fit a second order polynomial to each in pixel space\n",
    "        if space=='p': \n",
    "            left_coefficients = np.polyfit(lefty, leftx, 2)\n",
    "            right_coefficients = np.polyfit(righty, rightx, 2)\n",
    "        \n",
    "         # Fit a second order polynomial to each in real world space (metres)\n",
    "        elif space=='m': \n",
    "            left_coefficients = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "            right_coefficients = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "\n",
    "        return left_coefficients, right_coefficients\n",
    "    \n",
    "    \n",
    "    def windows_step(\n",
    "        self, \n",
    "        img, \n",
    "        nwindows, \n",
    "        leftx_current, \n",
    "        rightx_current, \n",
    "        nonzerox, \n",
    "        nonzeroy    \n",
    "    ): \n",
    "        \n",
    "        \"\"\"\n",
    "        Get pixels which are in good range of windows\n",
    "        \"\"\"\n",
    "        # Set height of windows\n",
    "        window_height = np.int(img.shape[0]/nwindows)\n",
    "        \n",
    "        # Set minimum number of pixels found to recenter window\n",
    "        minpix = 50\n",
    "\n",
    "        # Set the width of the windows +/- margin\n",
    "        margin = 100\n",
    "        \n",
    "        # Create empty lists to receive left and right lane pixel indices\n",
    "        left_lane_inds = []\n",
    "        right_lane_inds = []\n",
    "\n",
    "        # Step through the windows one by one\n",
    "        for window in range(nwindows):\n",
    "\n",
    "            # Identify window boundaries in x and y (and right and left)\n",
    "            win_y_low = img.shape[0] - (window+1)*window_height\n",
    "            win_y_high = img.shape[0] - window*window_height\n",
    "\n",
    "            win_xleft_low = leftx_current - margin\n",
    "            win_xleft_high = leftx_current + margin\n",
    "\n",
    "            win_xright_low = rightx_current - margin\n",
    "            win_xright_high = rightx_current + margin\n",
    "\n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_left_inds = (\n",
    "                (nonzeroy >= win_y_low) \n",
    "                & (nonzeroy < win_y_high) \n",
    "                & (nonzerox >= win_xleft_low) \n",
    "                & (nonzerox < win_xleft_high)\n",
    "            ).nonzero()[0]\n",
    "\n",
    "            good_right_inds = (\n",
    "                (nonzeroy >= win_y_low) \n",
    "                & (nonzeroy < win_y_high) \n",
    "                & (nonzerox >= win_xright_low) \n",
    "                & (nonzerox < win_xright_high)\n",
    "            ).nonzero()[0] \n",
    "\n",
    "            # Append these indices to the lists\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_left_inds) > minpix:\n",
    "                leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "\n",
    "            if len(good_right_inds) > minpix:        \n",
    "                rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "        # Concatenate the arrays of indices\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "        return (left_lane_inds, right_lane_inds)\n",
    "            \n",
    "    def get_lane_line_coefficients(self, img, ploty, nwindows=9, space='p'):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        Arguments: \n",
    "            img: \n",
    "                must be binary\n",
    "                must show a top down view of road\n",
    "            nwindows: \n",
    "                number of sliding windows to use\n",
    "        Returns: \n",
    "            left_coefficients: \n",
    "            right_coefficients:\n",
    "        \"\"\"\n",
    "\n",
    "        # Identify the x and y positions of all nonzero pixels in the image\n",
    "        nonzero = img.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        \n",
    "        # Take a histogram of the bottom half of the image\n",
    "        # and all we want to figure out is where to start drawing \n",
    "        # our lane lines from at the bottom of the image\n",
    "        # this isn't using any histogram function\n",
    "        # all its doing is returning an (ordered) array of the sum of each column in our image\n",
    "        histogram = np.sum(img[int(img.shape[0]/2):,:], axis=0)\n",
    "        \n",
    "        # Find the peak of the left and right halves of the histogram\n",
    "        # These will be the starting points for our left and right lines\n",
    "        if not self.first_frame_processed:\n",
    "            \n",
    "            self.get_lane_position(img)\n",
    "\n",
    "            # get lane position variables\n",
    "            midpoint = self.midpoint_lane_x\n",
    "            leftx_base = self.left_lane_x\n",
    "            rightx_base = self.right_lane_x\n",
    "\n",
    "            # Current positions to be updated for each window\n",
    "            leftx_current = leftx_base\n",
    "            rightx_current = rightx_base\n",
    "\n",
    "            left_lane_inds, right_lane_inds = self.windows_step(\n",
    "                img, \n",
    "                nwindows, \n",
    "                leftx_current, \n",
    "                rightx_current,\n",
    "                nonzerox, \n",
    "                nonzeroy\n",
    "            )\n",
    "\n",
    "            left_coefficients, right_coefficients = self.calc_lane_line_coefficients(\n",
    "                nonzerox, \n",
    "                nonzeroy,\n",
    "                left_lane_inds, \n",
    "                right_lane_inds, \n",
    "                histogram,\n",
    "                space='p', \n",
    "                ploty=None\n",
    "            )\n",
    "            \n",
    "            self.left_coefficients = left_coefficients\n",
    "            self.right_coefficients = right_coefficients\n",
    "            \n",
    "        else: \n",
    "                \n",
    "            # Assuming you have already a good fit for your lane lines from \n",
    "            # the first frame processed\n",
    "            # we don't need to do another blind search\n",
    "\n",
    "            margin = 100\n",
    "            \n",
    "            left_lane_inds = (\n",
    "                (nonzerox > (self.left_coefficients[0]*(nonzeroy**2) + \n",
    "                             self.left_coefficients[1]*nonzeroy + \n",
    "                             self.left_coefficients[2] - margin)) \n",
    "                & (nonzerox < (self.left_coefficients[0]*(nonzeroy**2) + \n",
    "                               self.left_coefficients[1]*nonzeroy + \n",
    "                               self.left_coefficients[2] + margin))\n",
    "            ) \n",
    "\n",
    "            right_lane_inds = (\n",
    "                (nonzerox > (self.right_coefficients[0]*(nonzeroy**2) + \n",
    "                             self.right_coefficients[1]*nonzeroy + \n",
    "                             self.right_coefficients[2] - margin)) \n",
    "                & (nonzerox < (self.right_coefficients[0]*(nonzeroy**2) + \n",
    "                               self.right_coefficients[1]*nonzeroy + \n",
    "                               self.right_coefficients[2] + margin))\n",
    "            )  \n",
    "\n",
    "            left_coefficients, right_coefficients = self.calc_lane_line_coefficients(\n",
    "                nonzerox, \n",
    "                nonzeroy, \n",
    "                left_lane_inds, \n",
    "                right_lane_inds, \n",
    "                histogram, \n",
    "                ploty=None,\n",
    "                space='p'\n",
    "            )\n",
    "            \n",
    "        self.first_frame_processed = True\n",
    "            \n",
    "        return left_coefficients, right_coefficients\n",
    "    \n",
    "    def calc_curvature(self, binary_warped_img, y_eval, ploty): \n",
    "        \n",
    "        # Fit new polynomials to x,y in world space\n",
    "        left_coefficients_m, right_coefficients_m = self.get_lane_line_coefficients(\n",
    "            binary_warped_img, ploty, space='m'\n",
    "        )\n",
    "        \n",
    "        # Calculate the new radii of curvature\n",
    "        left_curve_rad = ((1 + (2*left_coefficients_m[0]*y_eval*ym_per_pix + left_coefficients_m[1])**2)**1.5) / np.absolute(2*left_coefficients_m[0])\n",
    "        \n",
    "        right_curve_rad = ((1 + (2*right_coefficients_m[0]*y_eval*ym_per_pix + right_coefficients_m[1])**2)**1.5) / np.absolute(2*right_coefficients_m[0])\n",
    "\n",
    "        return left_curve_rad, right_curve_rad\n",
    "    \n",
    "    def draw_lane(self, og_img, binary_warped_img, Minv):\n",
    "        \"\"\"\n",
    "        Draw the lane lines on the image `img` using the poly `left_fit` and `right_fit`.\n",
    "        \"\"\"\n",
    "        \n",
    "        left_coefficients, right_coefficients = self.get_lane_line_coefficients(\n",
    "            binary_warped_img, ploty=None, space='p'\n",
    "        )\n",
    "        \n",
    "        yMax = og_img.shape[0]\n",
    "        ploty = np.linspace(0, yMax - 1, yMax)\n",
    "        color_warp = np.zeros_like(og_img).astype(np.uint8)\n",
    "\n",
    "        # Calculate points\n",
    "        left_fitx = left_coefficients[0]*ploty**2 + left_coefficients[1]*ploty + left_coefficients[2]\n",
    "        right_fitx = right_coefficients[0]*ploty**2 + right_coefficients[1]*ploty + right_coefficients[2]\n",
    "\n",
    "        # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "        pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "        pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "        # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "        newwarp = cv2.warpPerspective(color_warp, Minv, (og_img.shape[1], og_img.shape[0]))\n",
    "        \n",
    "        \n",
    "        # calculate curvature\n",
    "        left_curve_rad, right_curve_rad = self.calc_curvature(\n",
    "            binary_warped_img = binary_warped_img, \n",
    "            y_eval = np.max(ploty), \n",
    "            ploty = ploty\n",
    "        )\n",
    "        \n",
    "        # add curvature to image\n",
    "        cv2.putText(\n",
    "            img = og_img, \n",
    "            text = (\"Radius of curvature: %s\" %round(left_curve_rad, 0)), \n",
    "            org = (25,50), \n",
    "            fontFace = cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            fontScale = 1, \n",
    "            color = [255, 255, 255], \n",
    "            thickness = 5\n",
    "        )\n",
    "        \n",
    "        # add vehicle position wrt to left lane to image\n",
    "        cv2.putText(\n",
    "            img = og_img, \n",
    "            text = (\"Vehicle is %s m left of the center\" %round(left_curve_rad, 0)), \n",
    "            org = (25,100), \n",
    "            fontFace = cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            fontScale = 1, \n",
    "            color = [255, 255, 255], \n",
    "            thickness = 5\n",
    "        )\n",
    "\n",
    "        return cv2.addWeighted(og_img, 1, newwarp, 0.3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_line_detector = Lane_line_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_identified = lane_line_detector.draw_lane(test_image.copy(), warped.copy(), Minv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_list(\n",
    "    img_list=[\n",
    "        test_image, lane_identified, \n",
    "    ],\n",
    "    img_labels=[\n",
    "        'original', 'lane_identified'\n",
    "    ], \n",
    "    cmap='gray',\n",
    "    cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on images\n",
    "\n",
    "Now let's combine all of the above functions into one with our best parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_line_detector = Lane_line_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img, show_plots=False):\n",
    "    \"\"\"\n",
    "    One function to rule them all!\n",
    "    \"\"\"\n",
    "    \n",
    "    # undistory image \n",
    "    undistorted_image = undistort(img, mtx, dist)\n",
    "    \n",
    "    # convert image to hsl\n",
    "    hls_s_image = hls(img=undistorted_image, channel='s')\n",
    "    \n",
    "    # apply Gaussian smoothing\n",
    "    blur_image = gaussian_blur(hls_s_image)\n",
    "    \n",
    "    # threshold image\n",
    "    params=[\n",
    "#     color_threshold(\n",
    "#         img=blur_image, \n",
    "#         thresh=(50,255)\n",
    "#     ), \n",
    "     abs_sobel_thresh(\n",
    "        img = blur_image, \n",
    "        orient = 'y', \n",
    "        sobel_kernel = 11, \n",
    "        thresh = (10, 255)\n",
    "    ), \n",
    "     mag_threshold(\n",
    "         img = blur_image, \n",
    "         sobel_kernel = 11, \n",
    "         thresh = (15, 255)\n",
    "     ), \n",
    "#      dir_threshold(\n",
    "#          img = blur_image, \n",
    "#          sobel_kernel = 9, \n",
    "#          thresh = (0.7, 1.3)\n",
    "#      )\n",
    "    ]\n",
    "\n",
    "    threshold_image = combine_thresholds(\n",
    "        images = [param for param in params]\n",
    "    )\n",
    "    \n",
    "    # warp perspective to top down view\n",
    "    warped_image = adjustPerspective(threshold_image, M)\n",
    "    \n",
    "    # lane on image\n",
    "    final_image = lane_line_detector.draw_lane(img, warped_image, Minv)\n",
    "        \n",
    "    if show_plots==True:\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(image_hsl)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(image_gray, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(blur_gray, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "    else: \n",
    "        return final_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our function on all the images to see how well it worked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_images = list(map(lambda img: process_image(img, show_plots=False), \n",
    "                           original_images.copy()))\n",
    "\n",
    "show_image_list(img_list=all_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on videos\n",
    "\n",
    "Drawing lanes over images is pretty cool but drawing lanes over video is awesome!\n",
    "\n",
    "We'll test our solution on 3 different videos, ordered below by difficulty:\n",
    "\n",
    "1. `project_video.mp4`\n",
    "\n",
    "2. `challenge_video.mp4`\n",
    "\n",
    "3. `harder_challenge_video.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_filepath, video_filename, lane_detector, plot=False):\n",
    "    \n",
    "    video_input = VideoFileClip(video_filepath + video_filename)\n",
    "    video_output = video_input.fl_image(lane_detector)\n",
    "    video_output.write_videofile(video_filename , audio=False)\n",
    "    \n",
    "    HTML(\n",
    "        \"\"\"<video width=\"800\" height=\"400\" controls>\n",
    "        <source src=\"{0}\">\n",
    "        </video>\"\"\"\n",
    "        .format(video_output)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try the one with the solid white lane on the right first. This is a fairly simple video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(\"test_videos/\", \"project_video.mp4\", process_image, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the challenge video, one with the solid yellow lane on the left. This one's more tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(\"test_videos/\", \"challenge_video.mp4\", process_image, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lastly, we'll try the harder challenge video.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(\"test_videos/\", \"harder_challenge_video.mp4\", process_image, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humm the lines in our videos are quite jerky. Let's try and fix that. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
